{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcb481b-803e-465f-b35d-f35e53f455b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitdas/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/home/rohitdas/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import dlib\n",
    "import torchvision.transforms as transforms\n",
    "import yaml\n",
    "from models.stylegan3.model import GeneratorType\n",
    "from utils.common import tensor2im\n",
    "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
    "from utils.alignment_utils import align_face, crop_face, get_stylegan_transform\n",
    "from editing.interfacegan.face_editor import FaceEditor\n",
    "from ThreeDDFA_utils.uv import uv_tex\n",
    "from ThreeDDFA_utils.serialization import ser_to_obj\n",
    "from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n",
    "from TDDFA.TDDFA_ONNX import TDDFA_ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95b9a8-9f01-483c-8dbd-40f9181569a4",
   "metadata": {},
   "source": [
    "# Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91dd74e5-a0e1-4661-b8ec-6290bd8c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pSp_model_path = \"./pretrained_models/restyle_pSp_ffhq.pt\"\n",
    "e4e_model_path = \"./pretrained_models/restyle_e4e_ffhq.pt\"\n",
    "shape_predictor_path = \"./pretrained_models/shape_predictor_68_face_landmarks.dat\"\n",
    "cfg = yaml.load(open('ThreeDDFA_configs/mb1_120x120.yml'), Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a0ae3-9e3a-457f-a3dc-64efa578388a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ab153c-2b72-4dec-9bc1-25dc438f5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image\n",
    "face_img_path = 'input_data/00012.png' # Change the file name here\n",
    "pose_img_path = f'output_data/{face_img_path.split(\"/\")[-1].replace(\".png\", \"\")}_pose' + '.png' # Change the file name here\n",
    "uv_tex_path = f'output_data/{face_img_path.split(\"/\")[-1].replace(\".png\", \"\")}_uv_tex' + '.png'# Change the file name here\n",
    "obj_tex_path = f'output_data/{face_img_path.split(\"/\")[-1].replace(\".png\", \"\")}_obj' + '.obj'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58362122-cba1-4148-ae18-5163721672d6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c83b30-b0ab-41af-85d9-69e524b24f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load encoder type\n",
    "experiment_type = 'restyle_e4e_ffhq' # can choose between e4e and pSp encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1b424c-8610-4f25-909d-762c189d12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion iteration: Higher means getting closer. But sometimes 1 iteration produce good results\n",
    "n_iters_per_batch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f994fc81-3f53-4565-ac59-ff6fc99f2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose data processing\n",
    "edit_direction = 'pose'\n",
    "# Range of the pose Depends on how much you want the face to move\n",
    "min_value = -3\n",
    "max_value = 5\n",
    "# out of the range above, take the index which will generate the frontal face\n",
    "frontal_face_index = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c35c3-aa05-4799-9eb3-33d6dad07607",
   "metadata": {},
   "source": [
    "# Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a0e936-ee7e-4b61-ad00-b47f6e424632",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"restyle_pSp_ffhq\": {\n",
    "        \"model_path\": pSp_model_path,\n",
    "        \"image_path\": face_img_path,\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"restyle_e4e_ffhq\": {\n",
    "        \"model_path\": e4e_model_path,\n",
    "        \"image_path\": face_img_path,\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "070fb386-5ef2-4da0-9ae2-3728957430aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9dc72a-f7d1-432e-87c7-6a0ab375accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alignment(input_img):\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Aligning image...\")\n",
    "    aligned_image = align_face(input_img, detector=detector, predictor=predictor)\n",
    "    print(f\"Finished aligning image: {image_path}\")\n",
    "    return aligned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321e5b66-e75b-4672-83b5-d80685c75c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(input_img):\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Cropping image...\")\n",
    "    cropped_image = crop_face(input_img, detector=detector, predictor=predictor)\n",
    "    print(f\"Finished cropping image: {image_path}\")\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf00023-ad2c-40fb-b515-1a6fca2894e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transforms(aligned_img, cropped_img):\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Computing landmarks-based transforms...\")\n",
    "    res = get_stylegan_transform(cropped_img, aligned_img, detector, predictor)\n",
    "    print(\"Done!\")\n",
    "    if res is None:\n",
    "        print(f\"Failed computing transforms on: compute transform\")\n",
    "        return\n",
    "    else:\n",
    "        rotation_angle, translation, transform, inverse_transform = res\n",
    "        return inverse_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3a122-fdea-491a-be9b-e74d4f8d73c1",
   "metadata": {},
   "source": [
    "# Load the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f03ea1-af79-4a43-aebc-67ef5b7ba486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Github Repo/3DGANTex/utils/inference_utils.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ReStyle e4e from checkpoint: ./pretrained_models/restyle_e4e_ffhq.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Github Repo/3DGANTex/inversion/models/e4e3.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN3 generator from path: None\n",
      "Done!\n",
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_encoder(checkpoint_path=model_path)\n",
    "# pprint.pprint(dataclasses.asdict(opts))\n",
    "# Show the image\n",
    "image_path = str(EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"])\n",
    "original_image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd1d8523-d93a-44bb-a8e5-7fd8847459f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning image...\n",
      "Finished aligning image: input_data/00012.png\n",
      "Cropping image...\n",
      "Finished cropping image: input_data/00012.png\n"
     ]
    }
   ],
   "source": [
    "# Get aligned and cropped image\n",
    "aligned_image = run_alignment(original_image)\n",
    "cropped_image = crop_image(original_image)\n",
    "# cropped_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84c04edb-b919-4634-83af-f3213b5842b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing landmarks-based transforms...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Compute landmark based transform\n",
    "landmarks_transform = compute_transforms(aligned_image,cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "628a0a3a-7945-4824-9147-b8ae1a562c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitdas/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Inference took 0.7938 seconds.\n"
     ]
    }
   ],
   "source": [
    "#perform inversion\n",
    "n_iters_per_batch = 3\n",
    "opts.n_iters_per_batch = n_iters_per_batch\n",
    "opts.resize_outputs = False  # generate outputs at full resolution\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(original_image)\n",
    "avg_image = get_average_image(net)\n",
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
    "                                                net=net,\n",
    "                                                opts=opts,\n",
    "                                                avg_image=avg_image)\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d2cb75-5a5c-4a63-b05a-00a7ccf04f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tensors = result_batch[0]\n",
    "inversed_img = tensor2im(result_tensors[-1])\n",
    "inversed_img.save(pose_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dced1ea-90e2-47b7-bffd-5b47d15c4fbf",
   "metadata": {},
   "source": [
    "# Latent Space Editing using InterFaceGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8959900-d19e-4ff1-a687-0fb30865309c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '3D-GANTex/editing/interfacegan/boundaries/ffhq/age_boundary.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m editor \u001b[38;5;241m=\u001b[39m \u001b[43mFaceEditor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstylegan_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGeneratorType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALIGNED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming edit for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00medit_direction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m input_latent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(result_latents[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m/mnt/d/Github Repo/3DGANTex/editing/interfacegan/face_editor.py:22\u001b[0m, in \u001b[0;36mFaceEditor.__init__\u001b[0;34m(self, stylegan_generator, generator_type)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     paths \u001b[38;5;241m=\u001b[39m interfacegan_unaligned_edit_paths\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterfacegan_directions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmile\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmile\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mload(paths[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m     26\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:455\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    456\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '3D-GANTex/editing/interfacegan/boundaries/ffhq/age_boundary.npy'"
     ]
    }
   ],
   "source": [
    "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n",
    "\n",
    "\n",
    "print(f\"Performing edit for {edit_direction}...\")\n",
    "input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
    "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
    "                                        direction=edit_direction,\n",
    "                                        factor_range=(min_value, max_value),\n",
    "                                        user_transforms=landmarks_transform,\n",
    "                                        apply_user_transformations=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93648fc3-3183-4f0a-989c-2d41da856136",
   "metadata": {},
   "source": [
    "# Pose Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca1492-41b5-487c-8086-d0437b2214da",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_images = [image[0] for image in edit_images]\n",
    "res = np.array(edit_images[0].resize((512, 512)))\n",
    "frontal_face_img = np.asarray(edit_images[frontal_face_index]) # which image to choose\n",
    "# show pose images\n",
    "for image in edit_images[1:]:\n",
    "    res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
    "pose_img = Image.fromarray(res).convert(\"RGB\")\n",
    "# pose_img.show()\n",
    "pose_img.save(pose_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81094ce-7ac0-4c08-b090-3fcd8274020b",
   "metadata": {},
   "source": [
    "# Generate UV Map using 3DDFA_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e6689-6469-4094-a5a0-22104356b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_face_img_rgb = frontal_face_img[:, :, ::-1]\n",
    "# Init FaceBoxes and TDDFA, recommend using onnx flag\n",
    "face_boxes = FaceBoxes_ONNX()\n",
    "tddfa = TDDFA_ONNX(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a383f-047f-48fc-a69b-8624d8965f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Face\n",
    "boxes = face_boxes(frontal_face_img_rgb)\n",
    "param_lst, roi_box_lst = tddfa(frontal_face_img_rgb, boxes)\n",
    "ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb30405-7d93-45a3-bc3d-18104ac02600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UV_texture\n",
    "uv_tex(frontal_face_img_rgb,ver_lst,tddfa.tri,wfp= uv_tex_path )\n",
    "ser_to_obj(frontal_face_img_rgb, ver_lst, tddfa.tri,height = 1024, wfp=obj_tex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b55fa-4f6b-4508-af44-7dda4445250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the .obj file\n",
    "# import open3d as o3d\n",
    "\n",
    "# Read the OBJ file\n",
    "# mesh = o3d.io.read_triangle_mesh(obj_tex_path)\n",
    "\n",
    "# Visualize the mesh\n",
    "# o3d.visualization.draw_geometries([mesh])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
