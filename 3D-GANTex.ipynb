{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33171827-2cf8-4e0f-a378-685d866f902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitdas/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/home/rohitdas/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import dlib\n",
    "import torchvision.transforms as transforms\n",
    "import yaml\n",
    "from models.stylegan3.model import GeneratorType\n",
    "from utils.common import tensor2im\n",
    "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
    "from utils.alignment_utils import align_face, crop_face, get_stylegan_transform\n",
    "from editing.interfacegan.face_editor import FaceEditor\n",
    "from ThreeDDFA_utils.uv import uv_tex\n",
    "from ThreeDDFA_utils.serialization import ser_to_obj\n",
    "from FaceBoxes.FaceBoxes_ONNX import FaceBoxes_ONNX\n",
    "from TDDFA.TDDFA_ONNX import TDDFA_ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95b9a8-9f01-483c-8dbd-40f9181569a4",
   "metadata": {},
   "source": [
    "# Load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91dd74e5-a0e1-4661-b8ec-6290bd8c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Get the current working directory\n",
    "current_directory = Path.cwd()\n",
    "pSp_model_path = str(current_directory)+\"/pretrained_models/restyle_pSp_ffhq.pt\"\n",
    "e4e_model_path = str(current_directory)+\"/pretrained_models/restyle_e4e_ffhq.pt\"\n",
    "shape_predictor_path = str(current_directory)+\"/pretrained_models/shape_predictor_68_face_landmarks.dat\"\n",
    "threeddfa_configs_path = str(current_directory)+\"/ThreeDDFA_configs/mb1_120x120.yml\"\n",
    "# cfg = yaml.load(open(threeddfa_configs_path), Loader=yaml.SafeLoader)\n",
    "# Load YAML and create a new config with absolute paths while keeping original unchanged\n",
    "with open(threeddfa_configs_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    # Create absolute paths in memory without modifying the original file\n",
    "    cfg = {\n",
    "        **cfg,  # Unpack all original config\n",
    "        'checkpoint_fp': str(current_directory / cfg['checkpoint_fp']),\n",
    "        'bfm_fp': str(current_directory / cfg['bfm_fp'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a0ae3-9e3a-457f-a3dc-64efa578388a",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2ab153c-2b72-4dec-9bc1-25dc438f5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image\n",
    "face_img_path = 'input_data/01802.png' # Change the file name here\n",
    "pose_img_path = f'output_data/{face_img_path.split(\"/\")[-1].replace(\".png\", \"\")}_pose' + '.png' # Change the file name here\n",
    "uv_tex_path = f'output_data/{face_img_path.split(\"/\")[-1].replace(\".png\", \"\")}_uv_tex' + '.png'# Change the file name here\n",
    "obj_tex_path = f'output_data/{face_img_path.split(\"/\")[-1].replace(\".png\", \"\")}_obj' + '.obj'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58362122-cba1-4148-ae18-5163721672d6",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c83b30-b0ab-41af-85d9-69e524b24f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load encoder type\n",
    "experiment_type = 'restyle_e4e_ffhq' # can choose between e4e and pSp encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1b424c-8610-4f25-909d-762c189d12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion iteration: Higher means getting closer. But sometimes 1 iteration produce good results\n",
    "n_iters_per_batch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f994fc81-3f53-4565-ac59-ff6fc99f2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose data processing\n",
    "edit_direction = 'pose'\n",
    "# Range of the pose Depends on how much you want the face to move\n",
    "min_value = -3\n",
    "max_value = 5\n",
    "# out of the range above, take the index which will generate the frontal face\n",
    "frontal_face_index = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c35c3-aa05-4799-9eb3-33d6dad07607",
   "metadata": {},
   "source": [
    "# Define Inference Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a0e936-ee7e-4b61-ad00-b47f6e424632",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"restyle_pSp_ffhq\": {\n",
    "        \"model_path\": pSp_model_path,\n",
    "        \"image_path\": face_img_path,\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"restyle_e4e_ffhq\": {\n",
    "        \"model_path\": e4e_model_path,\n",
    "        \"image_path\": face_img_path,\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070fb386-5ef2-4da0-9ae2-3728957430aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9dc72a-f7d1-432e-87c7-6a0ab375accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alignment(input_img):\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Aligning image...\")\n",
    "    aligned_image = align_face(input_img, detector=detector, predictor=predictor)\n",
    "    print(f\"Finished aligning image: {image_path}\")\n",
    "    return aligned_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321e5b66-e75b-4672-83b5-d80685c75c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(input_img):\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Cropping image...\")\n",
    "    cropped_image = crop_face(input_img, detector=detector, predictor=predictor)\n",
    "    print(f\"Finished cropping image: {image_path}\")\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf00023-ad2c-40fb-b515-1a6fca2894e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transforms(aligned_img, cropped_img):\n",
    "    predictor = dlib.shape_predictor(shape_predictor_path)\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    print(\"Computing landmarks-based transforms...\")\n",
    "    res = get_stylegan_transform(cropped_img, aligned_img, detector, predictor)\n",
    "    print(\"Done!\")\n",
    "    if res is None:\n",
    "        print(f\"Failed computing transforms on: compute transform\")\n",
    "        return\n",
    "    else:\n",
    "        rotation_angle, translation, transform, inverse_transform = res\n",
    "        return inverse_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3a122-fdea-491a-be9b-e74d4f8d73c1",
   "metadata": {},
   "source": [
    "# Load the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f03ea1-af79-4a43-aebc-67ef5b7ba486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Github Repo/3D-GANTex/utils/inference_utils.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ReStyle e4e from checkpoint: /mnt/d/Github Repo/3D-GANTex/pretrained_models/restyle_e4e_ffhq.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Github Repo/3D-GANTex/inversion/models/e4e3.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StyleGAN3 generator from path: None\n",
      "Done!\n",
      "Model successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_encoder(checkpoint_path=model_path)\n",
    "# pprint.pprint(dataclasses.asdict(opts))\n",
    "# Show the image\n",
    "image_path = str(EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"])\n",
    "original_image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd1d8523-d93a-44bb-a8e5-7fd8847459f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning image...\n",
      "Finished aligning image: input_data/01802.png\n",
      "Cropping image...\n",
      "Finished cropping image: input_data/01802.png\n"
     ]
    }
   ],
   "source": [
    "# Get aligned and cropped image\n",
    "aligned_image = run_alignment(original_image)\n",
    "cropped_image = crop_image(original_image)\n",
    "# cropped_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84c04edb-b919-4634-83af-f3213b5842b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing landmarks-based transforms...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Compute landmark based transform\n",
    "landmarks_transform = compute_transforms(aligned_image,cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628a0a3a-7945-4824-9147-b8ae1a562c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitdas/anaconda3/envs/3DGANTex/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Inference took 0.6027 seconds.\n"
     ]
    }
   ],
   "source": [
    "#perform inversion\n",
    "n_iters_per_batch = 3\n",
    "opts.n_iters_per_batch = n_iters_per_batch\n",
    "opts.resize_outputs = False  # generate outputs at full resolution\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(original_image)\n",
    "avg_image = get_average_image(net)\n",
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
    "                                                net=net,\n",
    "                                                opts=opts,\n",
    "                                                avg_image=avg_image)\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d2cb75-5a5c-4a63-b05a-00a7ccf04f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tensors = result_batch[0]\n",
    "inversed_img = tensor2im(result_tensors[-1])\n",
    "inversed_img.save(pose_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dced1ea-90e2-47b7-bffd-5b47d15c4fbf",
   "metadata": {},
   "source": [
    "# Latent Space Editing using InterFaceGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8959900-d19e-4ff1-a687-0fb30865309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing edit for pose...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n",
    "\n",
    "\n",
    "print(f\"Performing edit for {edit_direction}...\")\n",
    "input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
    "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
    "                                        direction=edit_direction,\n",
    "                                        factor_range=(min_value, max_value),\n",
    "                                        user_transforms=landmarks_transform,\n",
    "                                        apply_user_transformations=True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93648fc3-3183-4f0a-989c-2d41da856136",
   "metadata": {},
   "source": [
    "# Pose Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32ca1492-41b5-487c-8086-d0437b2214da",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_images = [image[0] for image in edit_images]\n",
    "res = np.array(edit_images[0].resize((512, 512)))\n",
    "frontal_face_img = np.asarray(edit_images[frontal_face_index]) # which image to choose\n",
    "# show pose images\n",
    "for image in edit_images[1:]:\n",
    "    res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
    "pose_img = Image.fromarray(res).convert(\"RGB\")\n",
    "# pose_img.show()\n",
    "pose_img.save(pose_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81094ce-7ac0-4c08-b090-3fcd8274020b",
   "metadata": {},
   "source": [
    "# Generate UV Map using 3DDFA_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "303e6689-6469-4094-a5a0-22104356b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_face_img_rgb = frontal_face_img[:, :, ::-1]\n",
    "# Init FaceBoxes and TDDFA, recommend using onnx flag\n",
    "face_boxes = FaceBoxes_ONNX()\n",
    "tddfa = TDDFA_ONNX(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc8a383f-047f-48fc-a69b-8624d8965f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Face\n",
    "boxes = face_boxes(frontal_face_img_rgb)\n",
    "param_lst, roi_box_lst = tddfa(frontal_face_img_rgb, boxes)\n",
    "ver_lst = tddfa.recon_vers(param_lst, roi_box_lst, dense_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb30405-7d93-45a3-bc3d-18104ac02600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save visualization result to output_data/01802_uv_tex.png\n",
      "Dump to output_data/01802_obj.obj\n"
     ]
    }
   ],
   "source": [
    "#UV_texture\n",
    "uv_tex(frontal_face_img_rgb,ver_lst,tddfa.tri,wfp= uv_tex_path )\n",
    "ser_to_obj(frontal_face_img_rgb, ver_lst, tddfa.tri,height = 1024, wfp=obj_tex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "477b55fa-4f6b-4508-af44-7dda4445250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the .obj file\n",
    "# import open3d as o3d\n",
    "\n",
    "# Read the OBJ file\n",
    "# mesh = o3d.io.read_triangle_mesh(obj_tex_path)\n",
    "\n",
    "# Visualize the mesh\n",
    "# o3d.visualization.draw_geometries([mesh])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
